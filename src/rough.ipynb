{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda', 1) if torch.cuda.is_available else torch.device('cpu')\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPA(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension, device=DEVICE, dtype=DTYPE):\n",
    "        super(SDPA, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.query = nn.Linear(in_features=self.in_dimension, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.key = nn.Linear(in_features=self.in_dimension, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Linear(in_features=self.in_dimension, out_features=self.out_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        kq = math.sqrt(self.kq_dimension)\n",
    "        attn = torch.matmul(Q,K.permute(0,2,1))/kq\n",
    "        attn = F.softmax(attn, dim=0)\n",
    "        \n",
    "        Z = torch.matmul(attn,V)\n",
    "        return Z\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension, num_heads=8,\n",
    "                 device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(MHA, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.sdpa_head_list = nn.ModuleList([])\n",
    "        for _ in range(num_heads):\n",
    "            sdpa_head = SDPA(self.in_dimension, self.out_dimension, self.kq_dimension,\n",
    "                            device=DEVICE, dtype=DTYPE)\n",
    "            self.sdpa_head_list.append(sdpa_head)\n",
    "            \n",
    "        self.out = nn.Linear(in_features=num_heads*out_dimension, out_features=out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        y_list = []\n",
    "        for head in self.sdpa_head_list:\n",
    "            y = head(x)\n",
    "            y_list.append(y)\n",
    "        \n",
    "        Y = torch.cat(y_list, dim=-1)\n",
    "        Z = self.out(Y)\n",
    "        return Z\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension,\n",
    "                 num_heads=8, linear_stretch=2,\n",
    "                 device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_stretch = linear_stretch\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.mha = MHA(self.in_dimension, self.out_dimension, self.kq_dimension, self.num_heads,\n",
    "                       self.device, self.dtype)\n",
    "        \n",
    "        self.ff1 = nn.Linear(self.out_dimension, self.linear_stretch*self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.ff2 = nn.Linear(self.linear_stretch*self.out_dimension, self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        self.layernorm2 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        Y = self.mha(x)\n",
    "        Y = self.layernorm1(Y + residual) \n",
    "        \n",
    "        residual = Y\n",
    "        Z = self.ff1(Y)\n",
    "        Z = F.relu(Z)\n",
    "        Z = self.ff2(Z)\n",
    "        Z = self.layernorm2(Z + residual)\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15, 35])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  = torch.rand((3, 15, 35), device=DEVICE, dtype=DTYPE)\n",
    "l = EncoderLayer(35, 35, 50)\n",
    "b = l.forward(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 13])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [3, 4, 5]])\n",
    "l = nn.Embedding(6,13)\n",
    "b = l(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMER ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCODER(nn.Module):\n",
    "    def __init__(self, model_dimension, kq_dimension, vocab_size,\n",
    "                 num_heads=8, linear_stretch=2, num_layers=6,\n",
    "                 use_pos_enc=True, device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(ENCODER, self).__init__()\n",
    "        \n",
    "        self.model_dim = model_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.linear_stretch = linear_stretch\n",
    "        self.num_layers = num_layers\n",
    "        self.use_pos_enc = use_pos_enc\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.embd = nn.Embedding(vocab_size, self.model_dim, padding_idx=0,\n",
    "                                dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        self.layer_list = nn.ModuleList([])\n",
    "        for _ in range(self.num_layers):\n",
    "            layer = EncoderLayer(self.model_dim, self.model_dim, self.kq_dimension,\n",
    "                 num_heads=self.num_heads, linear_stretch=self.linear_stretch,\n",
    "                 device=self.device, dtype=self.dtype\n",
    "                )\n",
    "            self.layer_list.append(layer)\n",
    "        \n",
    "    \n",
    "    def positional_encoding(self, seq_len, d_model):\n",
    "        position = torch.arange(0, seq_len, dtype=self.dtype, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=self.dtype, device=self.device) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe = torch.zeros(seq_len, d_model, dtype=self.dtype, device=self.device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Z = self.embd(x) * torch.sqrt(torch.tensor(self.model_dim, dtype=self.dtype, device=self.device))\n",
    "        if self.use_pos_enc:\n",
    "            seq_len = x.size(1) \n",
    "            pe = self.positional_encoding(seq_len, self.model_dim)\n",
    "            Z = Z + pe\n",
    "        print(Z.shape)\n",
    "        for layer in self.layer_list:\n",
    "            Z = layer(Z)\n",
    "        return Z\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 15, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 500, (10,15), device=torch.device('cuda', 1))\n",
    "enc = ENCODER(128, 256, 500)\n",
    "b = enc.forward(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MUL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
