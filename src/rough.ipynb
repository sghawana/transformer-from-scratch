{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda', 1) if torch.cuda.is_available else torch.device('cpu')\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPA(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension,\n",
    "                masked=False, device=DEVICE, dtype=DTYPE):\n",
    "        super(SDPA, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        \n",
    "        self.masked = masked\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.query = nn.Linear(in_features=self.in_dimension, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.key = nn.Linear(in_features=self.in_dimension, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Linear(in_features=self.in_dimension, out_features=self.out_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        kq = math.sqrt(self.kq_dimension)\n",
    "        attn = torch.matmul(Q,K.permute(0,2,1))/kq\n",
    "        if self.masked:\n",
    "            attn_mask = torch.triu(torch.ones_like(attn), diagonal=1).bool()\n",
    "            attn.masked_fill_(attn_mask, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        Z = torch.matmul(attn,V)\n",
    "        return Z\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension, num_heads=8,\n",
    "                 masked=False, device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(MHA, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.masked = masked\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.sdpa_head_list = nn.ModuleList([])\n",
    "        for _ in range(num_heads):\n",
    "            sdpa_head = SDPA(self.in_dimension, self.out_dimension, self.kq_dimension,\n",
    "                            masked=self.masked, device=DEVICE, dtype=DTYPE)\n",
    "            self.sdpa_head_list.append(sdpa_head)\n",
    "            \n",
    "        self.out = nn.Linear(in_features=num_heads*out_dimension, out_features=out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        y_list = []\n",
    "        for head in self.sdpa_head_list:\n",
    "            y = head(x)\n",
    "            y_list.append(y)\n",
    "        \n",
    "        Y = torch.cat(y_list, dim=-1)\n",
    "        Z = self.out(Y)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension,\n",
    "                 num_heads=8, linear_stretch=2, dropout=0.1,\n",
    "                 device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_stretch = linear_stretch\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.mha = MHA(self.in_dimension, self.out_dimension, self.kq_dimension, self.num_heads,\n",
    "                       device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        self.ff1 = nn.Linear(self.out_dimension, self.linear_stretch*self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.ff2 = nn.Linear(self.linear_stretch*self.out_dimension, self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        self.layernorm2 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        self.dropoutL = nn.Dropout(self.dropout)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        Y = self.mha(x)\n",
    "        Y = self.layernorm1(Y + residual) \n",
    "        Y = self.dropoutL(Y)\n",
    "        \n",
    "        residual = Y\n",
    "        Z = self.ff1(Y)\n",
    "        Z = F.relu(Z)\n",
    "        Z = self.ff2(Z)\n",
    "        Z = self.layernorm2(Z + residual)\n",
    "        Z = self.dropoutL(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15, 35])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  = torch.rand((3, 15, 35), device=DEVICE, dtype=DTYPE)\n",
    "l = EncoderLayer(35, 35, 50)\n",
    "b = l.forward(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 13])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [3, 4, 5]])\n",
    "l = nn.Embedding(6,13)\n",
    "b = l(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMER ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCODER(nn.Module):\n",
    "    def __init__(self, encoder_dimension, kq_dimension, vocab_size, seq_len,\n",
    "                 num_heads=8, linear_stretch=2, num_layers=6, padding_index=0,\n",
    "                 use_pos_enc=True, device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(ENCODER, self).__init__()\n",
    "        \n",
    "        self.encoder_dim = encoder_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.linear_stretch = linear_stretch\n",
    "        self.num_layers = num_layers\n",
    "        self.use_pos_enc = use_pos_enc\n",
    "        self.padding_index = padding_index\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.embd = nn.Embedding(self.vocab_size, self.encoder_dim, padding_idx=self.padding_index,\n",
    "                                dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        self.layer_list = nn.ModuleList([])\n",
    "        for _ in range(self.num_layers):\n",
    "            layer = EncoderLayer(self.encoder_dim, self.encoder_dim, self.kq_dimension,\n",
    "                 num_heads=self.num_heads, linear_stretch=self.linear_stretch,\n",
    "                 device=self.device, dtype=self.dtype\n",
    "                )\n",
    "            self.layer_list.append(layer)\n",
    "        \n",
    "    \n",
    "    def positional_encoding(self):\n",
    "        position = torch.arange(0, self.seq_len, dtype=self.dtype, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.encoder_dim, 2, dtype=self.dtype, device=self.device) * -(torch.log(torch.tensor(10000.0)) / self.encoder_dim))\n",
    "        pe = torch.zeros(self.seq_len, self.encoder_dim, dtype=self.dtype, device=self.device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Z = self.embd(x) * torch.sqrt(torch.tensor(self.encoder_dim, dtype=self.dtype, device=self.device))\n",
    "        if self.use_pos_enc:\n",
    "            pe = self.positional_encoding()\n",
    "            Z = Z + pe\n",
    "        print(Z.shape)\n",
    "        for layer in self.layer_list:\n",
    "            Z = layer(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross SDPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_SDPA(nn.Module):\n",
    "    def __init__(self, enc_inp_dim, dec_inp_dim, out_dimension, kq_dimension,\n",
    "                device=DEVICE, dtype=DTYPE):\n",
    "        super(Cross_SDPA, self).__init__()\n",
    "        \n",
    "        self.enc_inp_dim = enc_inp_dim\n",
    "        self.dec_inp_dim = dec_inp_dim\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.query = nn.Linear(in_features=self.dec_inp_dim, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.key = nn.Linear(in_features=self.enc_inp_dim, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.value = nn.Linear(in_features=self.enc_inp_dim, out_features=self.out_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        Q = self.query(y)\n",
    "        K = self.key(z)\n",
    "        V = self.value(z)\n",
    "        \n",
    "        kq = math.sqrt(self.kq_dimension)\n",
    "        attn = torch.matmul(Q,K.permute(0,2,1))/kq\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        Y = torch.matmul(attn,V)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_MHA(nn.Module):\n",
    "    def __init__(self, enc_inp_dim, dec_inp_dim, out_dimension, kq_dimension, num_heads=8,\n",
    "                 device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(Cross_MHA, self).__init__()\n",
    "        \n",
    "        self.enc_inp_dim = enc_inp_dim\n",
    "        self.dec_inp_dim = dec_inp_dim\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.sdpa_head_list = nn.ModuleList([])\n",
    "        for _ in range(num_heads):\n",
    "            sdpa_head = Cross_SDPA(self.enc_inp_dim, self.dec_inp_dim, self.out_dimension,\n",
    "                                   self.kq_dimension, device=self.device, dtype=self.dtype\n",
    "                                )\n",
    "            self.sdpa_head_list.append(sdpa_head)\n",
    "            \n",
    "        self.out = nn.Linear(in_features=num_heads*out_dimension, out_features=out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "            \n",
    "    def forward(self, z, y):\n",
    "        w_list = []\n",
    "        for head in self.sdpa_head_list:\n",
    "            w = head(z, y)\n",
    "            w_list.append(w)\n",
    "        \n",
    "        W = torch.cat(w_list, dim=-1)\n",
    "        W = self.out(W)\n",
    "        return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, enc_inp_dim, dec_inp_dim, out_dimension, kq_dimension,\n",
    "                 num_heads=8, linear_stretch=2, dropout=0.1,\n",
    "                 device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.enc_inp_dim = enc_inp_dim\n",
    "        self.dec_inp_dim = dec_inp_dim\n",
    "        \n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_stretch = linear_stretch\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.masked_mha = MHA(self.dec_inp_dim, self.out_dimension, self.kq_dimension, self.num_heads,\n",
    "                                True, self.device, self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.cross_mha = Cross_MHA(self.enc_inp_dim, self.dec_inp_dim, self.out_dimension,\n",
    "                                   self.kq_dimension, self.num_heads, device=self.device,\n",
    "                                   dtype=self.dtype\n",
    "                                )\n",
    "        \n",
    "        self.ff1 = nn.Linear(self.out_dimension, self.linear_stretch*self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.ff2 = nn.Linear(self.linear_stretch*self.out_dimension, self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        self.layernorm2 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        self.layernorm3 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, e):\n",
    "        residual = x\n",
    "        Y = self.masked_mha(x)\n",
    "        Y = self.layernorm1(Y + residual) \n",
    "        Y = self.dropout(Y)\n",
    "        \n",
    "        residual = Y\n",
    "        Z = self.cross_mha(e, Y)\n",
    "        Z = self.layernorm2(Z + residual)\n",
    "        Z = self.dropout(Z)\n",
    "        \n",
    "        residual = Z\n",
    "        W = self.ff1(Z)\n",
    "        W = F.relu(W)\n",
    "        W = self.ff2(W)\n",
    "        W = self.layernorm3(W + residual)\n",
    "        W = self.dropout(W)\n",
    "        return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMER DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DECODER(nn.Module):\n",
    "    def __init__(self, decoder_dimension, encoder_dimension, kq_dimension, vocab_size,\n",
    "                 max_seq_len, num_heads=8, linear_stretch=2, num_layers=6, padding_index=0,\n",
    "                 use_pos_enc=True, dropout=0.1, device=DEVICE, dtype=DTYPE):\n",
    "        super(DECODER, self).__init__()\n",
    "        self.decoder_dim = decoder_dimension\n",
    "        self.encoder_dim = encoder_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_stretch = linear_stretch\n",
    "        self.num_layers = num_layers\n",
    "        self.use_pos_enc = use_pos_enc\n",
    "        self.padding_index = padding_index\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.embd = nn.Embedding(self.vocab_size, self.decoder_dim, padding_idx=self.padding_index,\n",
    "                                 dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        self.layer_list = nn.ModuleList([])\n",
    "        for _ in range(self.num_layers):\n",
    "            layer = DecoderLayer(self.encoder_dim, self.decoder_dim, self.decoder_dim, self.kq_dimension,\n",
    "                                 self.num_heads, self.linear_stretch, self.dropout,\n",
    "                                 self.device, self.dtype)\n",
    "            self.layer_list.append(layer)\n",
    "        \n",
    "        self.final = nn.Linear(self.decoder_dim, self.vocab_size,\n",
    "                               device=self.device, dtype=self.dtype)\n",
    "\n",
    "    def positional_encoding(self):\n",
    "        pe = torch.zeros(self.max_seq_len, self.decoder_dim, device=self.device, dtype=self.dtype)\n",
    "        position = torch.arange(0, self.max_seq_len, dtype=self.dtype, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.decoder_dim, 2, dtype=self.dtype, device=self.device) *\n",
    "                             -(math.log(10000.0) / self.decoder_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        Z = self.embd(x) * torch.sqrt(torch.tensor(self.decoder_dim, dtype=self.dtype, device=self.device))\n",
    "        if self.use_pos_enc:\n",
    "            pe = self.positional_encoding()\n",
    "            Z = Z + pe[:, :Z.size(1), :]\n",
    "        \n",
    "        for layer in self.layer_list:\n",
    "            Z = layer(Z, encoder_output)\n",
    "        \n",
    "        Y = self.final(Z)\n",
    "        Y = F.softmax(Y, dim=-1)\n",
    "        return Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MUL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
