{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda', 1)\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPA(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension, device=DEVICE, dtype=DTYPE):\n",
    "        super(SDPA, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.query = nn.Linear(in_features=self.in_dimension, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.key = nn.Linear(in_features=self.in_dimension, out_features=self.kq_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Linear(in_features=self.in_dimension, out_features=self.out_dimension,\n",
    "                               device=self.device, dtype=self.dtype\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        kq = math.sqrt(self.kq_dimension)\n",
    "        attn = torch.matmul(Q,K.permute(0,2,1))/kq\n",
    "        attn = F.softmax(attn, dim=0)\n",
    "        \n",
    "        Z = torch.matmul(attn,V)\n",
    "        return Z\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension, num_heads=8,\n",
    "                 device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(MHA, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.sdpa_head_list = nn.ModuleList([])\n",
    "        for _ in range(num_heads):\n",
    "            sdpa_head = SDPA(self.in_dimension, self.out_dimension, self.kq_dimension,\n",
    "                            device=DEVICE, dtype=DTYPE)\n",
    "            self.sdpa_head_list.append(sdpa_head)\n",
    "            \n",
    "        self.out = nn.Linear(in_features=num_heads*out_dimension, out_features=out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        y_list = []\n",
    "        for head in self.sdpa_head_list:\n",
    "            y = head(x)\n",
    "            y_list.append(y)\n",
    "        \n",
    "        Y = torch.cat(y_list, dim=-1)\n",
    "        Z = self.out(Y)\n",
    "        return Z\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension, kq_dimension,\n",
    "                 num_heads=8, linear_stretch=2,\n",
    "                 device=DEVICE, dtype=DTYPE\n",
    "                ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.kq_dimension = kq_dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_stretch = linear_stretch\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.mha = MHA(self.in_dimension, self.out_dimension, self.kq_dimension, self.num_heads,\n",
    "                       self.device, self.dtype)\n",
    "        \n",
    "        self.ff1 = nn.Linear(self.out_dimension, self.linear_stretch*self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.ff2 = nn.Linear(self.linear_stretch*self.out_dimension, self.out_dimension,\n",
    "                             device=self.device, dtype=self.dtype\n",
    "                            )\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        self.layernorm2 = nn.LayerNorm(self.out_dimension, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        Y = self.mha(x)\n",
    "        Y = self.layernorm1(Y + residual) \n",
    "        \n",
    "        residual = Y\n",
    "        Z = self.ff1(Y)\n",
    "        Z = F.relu(Z)\n",
    "        Z = self.ff2(Z)\n",
    "        Z = self.layernorm2(Z + residual)\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15, 35])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  = torch.rand((3, 15, 35), device=DEVICE, dtype=DTYPE)\n",
    "l = EncoderLayer(35, 35, 50)\n",
    "b = l.forward(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMER ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCODER(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ENCODER, self).__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MUL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
